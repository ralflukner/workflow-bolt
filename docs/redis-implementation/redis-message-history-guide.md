# Redis Message History – Retrieval & Analysis Guide

*Last updated: {{DATE}}*

This document explains how to pull past messages from the **agent_updates** Redis stream, convert them into readable formats, and perform basic analysis. It's intended for developers, testers, and operations engineers who need to audit or debug multi-agent interactions.

---

## 1. Prerequisites

1. **`REDIS_PASS` environment variable** is configured (see `REDIS_MESSAGING_STATUS.md`).  
2. Python ≥ 3.8 with the `redis` package installed:  
   ```bash
   python3 -m pip install redis
   ```
3. The helper utilities from `ai-agents/redis_event_bus.py` are on your `PYTHONPATH` (running from repo-root is sufficient).

---

## 2. Quick Snapshot (latest *N* messages)

```
python3 -m ai-agents.redis_event_bus read --count 100
```

* `--count N` – number of entries to fetch (default = 5).  
* `--last_id ID` – start after a specific stream ID (for pagination).

Sample output:

```text
--- Message ID: 1718142810123-0 ---
   sender: o3-max
   action: offer_help
   payload: {"to":"claude", ...}

--- Message ID: 1718142796555-0 ---
   sender: claude
   action: broadcast
   payload: {...}
```

---

## 3. Export to JSON for Deep-Dive

```bash
python3 - <<'PY'
from ai_agents.redis_event_bus import connect_to_redis, read_messages
import json, pathlib, datetime
r = connect_to_redis()
msgs = read_messages(r, count=1000)   # adjust count
outfile = pathlib.Path(
    f"redis-history-{datetime.datetime.utcnow().isoformat(timespec='seconds')}.json"
)
outfile.write_text(json.dumps(msgs, indent=2))
print(f"Wrote {len(msgs)} messages to {outfile}")
PY
```

Now you can:

* Load the file into a Jupyter notebook for charts.
* Grep for specific actions (`broadcast`, `request`, `response`).
* Follow a full request/response chain via `correlation_id`.

---

## 4. Building a Human-Readable Timeline

Because each enhanced message contains `sender`, `message_type`, and `correlation_id`, you can transform the JSON into a Markdown table:

| Time (UTC) | Sender | Type | Details |
|------------|--------|------|---------|
| 12:00:01   | React-frontend | request | `health_check` — cid=abc123 |
| 12:00:02   | backend-worker | response | OK (redis, php, tebra) cid=abc123 |
| 12:04:17   | o3-max | offer_help | → claude |
| 12:05:01   | claude | broadcast | "Starting integration tests" |

A simple Pandas script or `jq` + `awk` can generate this.

---

## 5. Live Tail ("follow mode")

```bash
python3 - <<'PY'
from ai_agents.redis_event_bus import connect_to_redis
import time, json
r = connect_to_redis()
last = "0-0"
print("Streaming new messages…  (Ctrl-C to exit)")
while True:
    msgs = r.xread({"agent_updates": last}, block=5000, count=10)
    if msgs:
        for _, mlist in msgs:
            for mid, data in mlist:
                last = mid
                print(mid, json.dumps({k.decode(): v.decode() for k,v in data.items()}))
PY
```

---

## 6. Troubleshooting

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| `ModuleNotFoundError: redis` | Redis client not installed | `python3 -m pip install redis` |
| `NOAUTH Authentication required` | `REDIS_PASS` not set | Ensure export line in `~/.zshrc`, then `source ~/.zshrc` |
| `NOGROUP` errors | Using `xreadgroup` without creating group | Use plain `xread` as shown above or create the group first |
| No output | Wrong DB / stream name | Confirm `STREAM_NAME` in `redis_event_bus.py` matches production |

---

## 7. Automating Regular Dumps

Add a cron or GitHub Action that runs the JSON export daily and saves artifacts, enabling long-term auditing.

```bash
0 2 * * * /usr/bin/python3 /path/to/repo/scripts/dump_redis_history.py --count 10000 --out /var/log/redis-history/$(date +\%F).json
```

A template script lives at `prj-mgmt/templates/testing/statistical-analysis/redis-history-dump-template.py` (generated by the testing framework).

---

*End of guide*
